{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third part of the architecture for gaze target detection in the wild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu for inference\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Using {device} for inference')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## At the end of the 3D Gaze Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def planar_angular_loss(dx, dy, gx, gy):\n",
    "    \"\"\"\n",
    "        d=(dx, dy) : ground truth planar gaze direction\n",
    "        g=(gx, gy) : 2D projection of the estimated 3D gaze direction\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    res = 1 - (dx*gx+dy*gy)/(((dx**2 + dy**2)**(1/2))*(gx**2 + gy**2)**(1/2))    \n",
    "    \n",
    "    return res "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dual Attention Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dual_attention_mode(Mf, Md):\n",
    "    \"\"\"\n",
    "        Mf : FOV   Attention Map\n",
    "        Md : Depth Attention Map\n",
    "        \n",
    "        return -> Mdual : Dual Attention Map \n",
    "    \"\"\"\n",
    "    return Mf*Md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backbone part (don't forget to activate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50 loaded\n"
     ]
    }
   ],
   "source": [
    "# ResNet50\n",
    "import torchvision.models as models\n",
    "backbone = models.resnet50(pretrained=True, progress=True)\n",
    "print(\"ResNet50 loaded\")\n",
    "# Freeze the learning of the layers of the resnet50\n",
    "for param in backbone.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchsummary import summary\n",
    "\n",
    "#out_ftrs, in_ftrs, bias_ftrs = backbone.fc.out_features, backbone.fc.in_features, len(backbone.fc.bias)\n",
    "\n",
    "#print(\"Parameters of the last layer of the backbone : \"out_ftrs, \",\", in_ftrs, \",\", bias_ftrs)\n",
    "#print(\"\\n\")\n",
    "#print(summary(backbone, (3,224,224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To prepare the data if necessary but normaly it's not\n",
    "preprocess = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(p1, p2, axis=0):\n",
    "    \"\"\"\n",
    "        p1, p2 : already tensors\n",
    "        axis   : 0 for raw, 1 for columns\n",
    "        \n",
    "        Concatenation of the two pictures of the entry\n",
    "    \"\"\"\n",
    "    return tf.concat([p1, p2], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1623459064158/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3879, -1.1348]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "class binary(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(binary, self).__init__()\n",
    "        \n",
    "        #Resnet\n",
    "        self.backbone = backbone\n",
    "        \n",
    "        #convolution layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, padding=\"same\")\n",
    "        self.conv2 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, padding=\"same\")\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc1 = nn.Linear(in_features=1000, out_features=2)\n",
    "        \n",
    "    # x represents our data\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        \n",
    "        x = x.unsqueeze(dim=1) #unsqueeze by adding the second dimension (in_channels=1)\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = x.squeeze(dim=1)\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "##########################################################\n",
    "#  Only a simple test on the Binary Classification Head  #\n",
    "##########################################################    \n",
    "    \n",
    "# Equates to one random 448x224 image\n",
    "random_data = torch.rand((1, 3, 224+224, 224))\n",
    "\n",
    "my_nn = binary()\n",
    "result = my_nn(random_data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap Regression Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class h_reg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(h_reg, self).__init__()\n",
    "        \n",
    "        #Resnet\n",
    "        self.backbone = backbone\n",
    "        \n",
    "        #convolution layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=10, stride=2, padding=6)\n",
    "        self.conv2 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=6)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.downsample1 = nn.ConvTranspose1d(in_channels=1, out_channels=1, kernel_size=4, stride=2, padding=1)\n",
    "        self.downsample2 = nn.ConvTranspose1d(in_channels=1, out_channels=1, kernel_size=4, stride=2, padding=1)\n",
    "        self.downsample3 = nn.ConvTranspose1d(in_channels=1, out_channels=1, kernel_size=4, stride=2, padding=1)\n",
    "        \n",
    "        \n",
    "    # x represents our data\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.backbone(x)\n",
    "       \n",
    "        x = x.unsqueeze(dim=1) \n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        x = self.downsample1(x)\n",
    "        x = self.downsample2(x)\n",
    "        x = self.downsample3(x)\n",
    "        \n",
    "        x = x.squeeze(dim=1)\n",
    "\n",
    "        output = F.softmax(x, dim=1)\n",
    "        output = output.reshape(1, 64, 64)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    \n",
    "##########################################################\n",
    "#   Only a simple test on the Heatmap Regression Head    #\n",
    "##########################################################\n",
    "\n",
    "# Equates to one random 448x224 image\n",
    "random_data = torch.rand((1, 3, 224+224, 224))\n",
    "\n",
    "my_nn = h_reg()\n",
    "result = my_nn(random_data)\n",
    "\n",
    "result = result.detach().numpy() \n",
    "#print(result)\n",
    "\n",
    "#plt.imshow(result[0,:,:])\n",
    "#plt.colorbar()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function for both of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_dl, val_dl, epochs=100, device='cpu'):\n",
    "\n",
    "    print('train() called: model=%s, opt=%s(lr=%f), epochs=%d, device=%s\\n' % \\\n",
    "          (type(model).__name__, type(optimizer).__name__,\n",
    "           optimizer.param_groups[0]['lr'], epochs, device))\n",
    "\n",
    "\n",
    "    history             = {} # Collects per-epoch loss and acc like Keras' fit().\n",
    "    history['loss']     = []\n",
    "    history['val_loss'] = []\n",
    "    history['acc']      = []\n",
    "    history['val_acc']  = []\n",
    "\n",
    "    start_time_sec = time.time()\n",
    "\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "\n",
    "        # --- TRAIN AND EVALUATE ON TRAINING SET -----------------------------\n",
    "        model.train()\n",
    "        train_loss         = 0.0\n",
    "        num_train_correct  = 0\n",
    "        num_train_examples = 0\n",
    "\n",
    "        for batch in train_dl:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x    = batch[0].to(device)\n",
    "            y    = batch[1].to(device)\n",
    "            yhat = model(x)\n",
    "            loss = loss_fn(yhat, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss         += loss.data.item() * x.size(0)\n",
    "            num_train_correct  += (torch.max(yhat, 1)[1] == y).sum().item()\n",
    "            num_train_examples += x.shape[0]\n",
    "\n",
    "        train_acc   = num_train_correct / num_train_examples\n",
    "        train_loss  = train_loss / len(train_dl.dataset)\n",
    "\n",
    "\n",
    "        # --- EVALUATE ON VALIDATION SET -------------------------------------\n",
    "        model.eval()\n",
    "        val_loss         = 0.0\n",
    "        num_val_correct  = 0\n",
    "        num_val_examples = 0\n",
    "\n",
    "        for batch in val_dl:\n",
    "\n",
    "            x    = batch[0].to(device)\n",
    "            y    = batch[1].to(device)\n",
    "            yhat = model(x)\n",
    "            loss = loss_fn(yhat, y)\n",
    "\n",
    "            val_loss         += loss.data.item() * x.size(0)\n",
    "            num_val_correct  += (torch.max(yhat, 1)[1] == y).sum().item()\n",
    "            num_val_examples += y.shape[0]\n",
    "\n",
    "        val_acc  = num_val_correct / num_val_examples\n",
    "        val_loss = val_loss / len(val_dl.dataset)\n",
    "\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('Epoch %3d/%3d, train loss: %5.2f, train acc: %5.2f, val loss: %5.2f, val acc: %5.2f' % \\\n",
    "            (epoch, epochs, train_loss, train_acc, val_loss, val_acc))\n",
    "\n",
    "        history['loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "    # END OF TRAINING LOOP\n",
    "\n",
    "    end_time_sec       = time.time()\n",
    "    total_time_sec     = end_time_sec - start_time_sec\n",
    "    time_per_epoch_sec = total_time_sec / epochs\n",
    "    print()\n",
    "    print('Time total:     %5.2f sec' % (total_time_sec))\n",
    "    print('Time per epoch: %5.2f sec' % (time_per_epoch_sec))\n",
    "\n",
    "    return history\n",
    "\n",
    "cc = history['acc']\n",
    "val_acc = history['val_acc']\n",
    "loss = history['loss']\n",
    "val_loss = history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
