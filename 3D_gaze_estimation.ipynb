{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3D gaze estimation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linainsaf/Dual-Attention-Guided-Gaze-Target-Detection-in-the-Wild/blob/main/3D_gaze_estimation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH3uuXFQVuMw"
      },
      "source": [
        "## Depth Estimation Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMGL0yrDVrmP"
      },
      "source": [
        "import cv2\n",
        "import torch\n",
        "import urllib.request\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gazefollow 3D Gaze estimation"
      ],
      "metadata": {
        "id": "2jWtyCN35eVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision import models \n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "from matplotlib import cm\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from torchvision import datasets\n",
        "import math\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzc-jswdLh5K",
        "outputId": "aaf6aba6-ef00-4240-9db0-264bd72ab695"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data and models"
      ],
      "metadata": {
        "id": "r4S4WqqJMNmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "heads = pickle.load( open( \"drive/MyDrive/MLA/imgs_heads_gaze_follow.pkl\", \"rb\" ) )\n",
        "\n",
        "removed =  []\n",
        "for i in range(len(heads)):\n",
        "  if  heads[i].any():\n",
        "      heads[i] = cv2.resize(heads[i], dsize=(224, 224), interpolation=cv2.INTER_CUBIC)\n",
        "  else : \n",
        "      removed.append(i)\n",
        "print(removed)"
      ],
      "metadata": {
        "id": "kF5INkUz5qHX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05ad1517-f308-48db-acca-3f076f09906e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[22, 2026]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRyiWet9guRP"
      },
      "source": [
        "eyes_l,eyes_r = pickle.load( open( \"drive/MyDrive/MLA/eyes.pkl\", \"rb\" ) )"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del heads[removed[0]]\n",
        "del heads[removed[1]-1]\n",
        "del heads[0]\n",
        "\n",
        "del eyes_l[0]\n",
        "del eyes_r[0]\n",
        "\n",
        "len(eyes_r),len(eyes_l), len(heads)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-BzRy0r_gEA",
        "outputId": "dc69f931-b8dd-412e-fcf7-de57d661b587"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3997, 3997, 3997)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DHMj6xkguOa"
      },
      "source": [
        "model_hp = torch.load(\"drive/MyDrive/MLA/head_pose_extractor.pth\")#, map_location='cpu')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createModel_resnet34(out_1, out_2):\n",
        "    model = torchvision.models.resnet34(pretrained = True)\n",
        "    num_ftrs = model.fc.in_features\n",
        "\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    \n",
        "    #Creating 3 Linear connected layers which can be trained\n",
        "    fc1 = nn.Linear(num_ftrs, out_1)\n",
        "    fc2 = nn.Linear(out_1, out_2)\n",
        "    fc3 = nn.Linear(out_2, 2)\n",
        "\n",
        "    layers = [fc1, fc2, fc3]\n",
        "    for linearLayer in layers:\n",
        "        #Applying He initialization to all layers\n",
        "        nn.init.kaiming_uniform_(linearLayer.weight, nonlinearity='leaky_relu')\n",
        "  \n",
        "\n",
        "\n",
        "    #Setting Resnet's fully connected layer to our collection of three Linear layers with nn.Sequential\n",
        "    model.fc = nn.Sequential(fc1, nn.LeakyReLU(),fc2, nn.LeakyReLU(),fc3)\n",
        "    model.double() #double to set variables to double\n",
        "    #Sending the device to the GPU if avaliable\n",
        "    model.to(device)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "KArHXZjdLTW1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createModel_MLP(hidden,tensor_size):\n",
        "    model = nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(tensor_size, hidden),\n",
        "      nn.ReLU(),\n",
        "      #nn.Linear(hidden, 128),\n",
        "      #nn.ReLU(),\n",
        "      nn.Linear(hidden, 3 ),\n",
        "    )\n",
        "    \n",
        "    model.double() #double to set variables to double\n",
        "    #Sending the device to the GPU if avaliable\n",
        "    model.to(device)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "13BX5D1vROcY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class eyeDataset(Dataset):\n",
        "    \"\"\"eye landmark dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, imgs, transform=None):\n",
        "       \n",
        "        self.imgs = imgs\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        #img_name = os.path.join(self.root_dir,self.landmarks_frame.iloc[idx, 0])\n",
        "        image = self.imgs[idx] #io.imread(img_name)\n",
        "        image = Image.fromarray(image)\n",
        "        sample = {'image': image}\n",
        "\n",
        "        if self.transform:\n",
        "            sample['image'] = self.transform(sample['image'])\n",
        "\n",
        "        return sample"
      ],
      "metadata": {
        "id": "tUwKeHVjT7bs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_eye_features(left_eye_imgs, right_eye_imgs):\n",
        "    # model import\n",
        "    resnet = models.resnet18(pretrained=True)\n",
        "    resnet18 = nn.Sequential(*(list(resnet.children())[:-1])) #take 8 layers \n",
        "    resnet18.to(device)\n",
        "    # transfor data\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )])\n",
        "\n",
        "    # left eye \n",
        "    l_eye_set=eyeDataset(imgs=left_eye_imgs,transform=preprocess)\n",
        "    l_eye_loader = DataLoader(l_eye_set, batch_size=571, shuffle=False, num_workers=2)\n",
        "    tensor_size = 512\n",
        "    features_left =torch.zeros((1,tensor_size), dtype=torch.int32, device = 'cuda')\n",
        "\n",
        "    for i in range(len(l_eye_loader)):\n",
        "        l=next(iter(l_eye_loader))\n",
        "        outputs_l=l['image'].to(device)\n",
        "        left=resnet18(outputs_l).flatten(start_dim=1)\n",
        "        features_left = torch.cat((features_left,left), 0)\n",
        "\n",
        "    features_left=features_left[1:,:]\n",
        "\n",
        "    # right eye\n",
        "    r_eye_set=eyeDataset(imgs=right_eye_imgs,transform=preprocess)\n",
        "    r_eye_loader = DataLoader(r_eye_set, batch_size=571, shuffle=False, num_workers=2)\n",
        "    resnet18.eval()\n",
        "    tensor_size = 512 #2048\n",
        "    features_right =torch.zeros((1,tensor_size), dtype=torch.int32, device = 'cuda')\n",
        "    for i in range(len(r_eye_loader)):\n",
        "        r=next(iter(r_eye_loader))\n",
        "        outputs_r=r['image'].to(device)\n",
        "        right=resnet18(outputs_r).flatten(start_dim=1)# yields a tensor of size([batch_size, 2048])\n",
        "        features_right = torch.cat((features_right,right), 0)\n",
        "\n",
        "    features_right=features_right[1:,:]\n",
        "\n",
        "    return features_left, features_right\n"
      ],
      "metadata": {
        "id": "nCeLj0cUUIoj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class headposeDataset(Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, imgs, transform=None):\n",
        "        self.imgs = imgs\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        image = self.imgs[idx] \n",
        "        image = Image.fromarray(image)\n",
        "        '''\n",
        "        landmarks = np.zeros(2)\n",
        "        landmarks=np.zeros(2)\n",
        "        landmarks[0]= ex[0]\n",
        "        landmarks[1]= ex[1]\n",
        "        '''\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        sample = {'image': image}\n",
        "\n",
        "        return sample"
      ],
      "metadata": {
        "id": "mRJGY5QPVxXR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPDataset(Dataset):\n",
        "    \"\"\"prep_data_for_MLP.\"\"\"\n",
        "\n",
        "    def __init__(self, data_in, transform=None):\n",
        "        self.data_in = data_in\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_in)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        x = self.data_in[idx]\n",
        "\n",
        "        if self.transform:\n",
        "              x = self.transform(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "7LePde4o1Y9q"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def head_pose_extraction(head_imgs):\n",
        "  batch_size= 128 #32\n",
        "  preprocess = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.double()),\n",
        "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "  \n",
        "  head_tensor = headposeDataset(imgs=head_imgs, transform=preprocess)\n",
        "  head_tensor_batched = torch.utils.data.DataLoader(head_tensor, batch_size=batch_size, num_workers=2)\n",
        "\n",
        "  # head pose prediction\n",
        "  model_hp = createModel_resnet34(512, 128)\n",
        "  model_hp.load_state_dict(torch.load(\"drive/MyDrive/MLA/head_pose_extractor.pth\"))#,map_location=torch.device('cpu')))\n",
        "  model_hp.eval() \n",
        "\n",
        "  prediction_hp =torch.zeros((1,2), dtype=torch.int32, device = 'cuda')\n",
        "\n",
        "  for i, batch in enumerate(head_tensor_batched):\n",
        "    prediction = model_hp(batch[\"image\"].to(device))\n",
        "    prediction_hp = torch.cat((prediction_hp,prediction), 0)\n",
        "  prediction_hp=prediction_hp[1:,:]\n",
        "  return prediction_hp\n"
      ],
      "metadata": {
        "id": "Xo1shlw13j6M"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we multiply this operator with the features extracted from the resnet\n",
        "def l_op(l_imgs,r_imgs): \n",
        "    l_imgs = torch.as_tensor(np.array(l_imgs)).cuda()\n",
        "    r_imgs = torch.as_tensor(np.array(r_imgs)).cuda()\n",
        "    eyes_b =l_imgs.sum(axis = (3,2,1)) +r_imgs.sum(axis = (3,2,1))\n",
        "    lop = (eyes_b).bool()\n",
        "    lop = lop.int()\n",
        "    return lop\n",
        "\n",
        "def lop_eyes_features( lop, eyes_features):\n",
        "    x= eyes_features\n",
        "    for i in range(len(lop)):\n",
        "      x[i] = torch.mul(eyes_features[i],lop[i])\n",
        "    return x\n",
        "\n",
        "\n",
        "def mlp_gaze_estimation(prediction_hp, features_left, features_right, left_eye_imgs, right_eye_imgs):\n",
        "  batch_size= 128\n",
        "  #prepare head position input\n",
        "  h = torch.as_tensor(prediction_hp).cuda()\n",
        "  #prepare eyes features : concatenation + multiplication with l operator \n",
        "  eyes_features = torch.cat((features_left, features_right),1)\n",
        "  lop = l_op(left_eye_imgs, right_eye_imgs)\n",
        "  # Multiply eyes features and L operator\n",
        "  lop_EyesFeatures = lop_eyes_features(lop,eyes_features)\n",
        "  # Concatenate eyes features with h\n",
        "  input_data = torch.cat((h,lop_EyesFeatures), 1)\n",
        "  #Normalize the concatenated tensor\n",
        "  input= input_data.clone().detach()\n",
        "  input = ((input.T - input.mean(axis = 1))/input.std(axis = 1)).T\n",
        "\n",
        "  #preprocess data \n",
        "  preprocess = transforms.Compose([transforms.Lambda(lambda x: x.double())])\n",
        "  dataset = MLPDataset(input, transform=preprocess)\n",
        "  dataset_batched = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=2)\n",
        "\n",
        "  #Model import\n",
        "  tensor_size= len(features_left[0])*2+len(prediction_hp[0])\n",
        "  model_gaze = createModel_MLP(256,tensor_size)\n",
        "  model_gaze.load_state_dict(torch.load(\"drive/MyDrive/MLA/MLP.pth\"))#,map_location=torch.device('cpu')))\n",
        "  model_gaze.eval() \n",
        "\n",
        "  gaze_prediction =torch.zeros((1,2), dtype=torch.int32, device = 'cuda')\n",
        "\n",
        "  for i, batch in enumerate(dataset_batched):\n",
        "    prediction = model_gaze(batch.to(device))\n",
        "    gaze_prediction = torch.cat((gaze_prediction,prediction), 0)\n",
        "  gaze_prediction = gaze_prediction[1:,:]\n",
        "\n",
        "  return gaze_prediction "
      ],
      "metadata": {
        "id": "WnsPywwT1iRT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gaze_estimation(head_imgs, left_eye_imgs, right_eye_imgs):\n",
        "    # head pose prediction\n",
        "    prediction_hp =  head_pose_extraction(head_imgs)\n",
        "    print(\"head pose estimated\")\n",
        "\n",
        "    # Get eye features\n",
        "    features_left, features_right = get_eye_features(left_eye_imgs, right_eye_imgs)\n",
        "    print(\"eyes features extracted\")\n",
        "\n",
        "    # gaze prediction\n",
        "    gaze_prediction = mlp_gaze_estimation(prediction_hp, features_left, features_right, left_eye_imgs, right_eye_imgs)\n",
        "    print(\"3D gaze predicted\")\n",
        "\n",
        "    return gaze_prediction\n"
      ],
      "metadata": {
        "id": "T_etmim9AdlP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = gaze_estimation(heads, eyes_l, eyes_r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnI7tAq0-OA6",
        "outputId": "71d1486b-b862-4c93-9c6b-e9a3df305660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "head pose estimated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ad5RY1y4QFDz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}