{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "gaze360gazeEstimation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GhwFIBUN68rf",
        "QcZpMbJD7fhI",
        "m9hiKiKjJ3jt"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEw1NOesH7jD",
        "outputId": "cfb1db2a-b3ee-441f-8eca-b2aecfc3a719"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "ZEw1NOesH7jD",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlQTZHoTJJHw",
        "outputId": "f1eb02d2-7f87-45fe-8ce4-d76a56449903"
      },
      "source": [
        "!pip install patool"
      ],
      "id": "wlQTZHoTJJHw",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting patool\n",
            "  Downloading patool-1.12-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▎                           | 10 kB 20.3 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 20 kB 24.1 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 30 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 40 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 51 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 61 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 71 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 77 kB 3.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: patool\n",
            "Successfully installed patool-1.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFQoG16yID5Q",
        "outputId": "194238a5-e7c4-4647-b78f-144ef9fd883f"
      },
      "source": [
        "import patoolib\n",
        "patoolib.extract_archive(\"/content/drive/MyDrive/gaze360.rar\", outdir=\"/content\")"
      ],
      "id": "GFQoG16yID5Q",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "patool: Extracting /content/drive/MyDrive/gaze360.rar ...\n",
            "patool: running /usr/bin/unrar x -- /content/drive/MyDrive/gaze360.rar\n",
            "patool:     with cwd='/content'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElYGjtHRJDuo"
      },
      "source": [
        "from first_part_functions import Kellnhofer_al_eye, Kellnhofer_al_eyes, bulat_al, head_eye_extractors"
      ],
      "id": "ElYGjtHRJDuo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47784911",
        "outputId": "73118b5a-6a06-4079-9f0b-f503c1351fe3"
      },
      "source": [
        "import numpy as np\n",
        "from os.path import dirname, join as pjoin\n",
        "import scipy.io as sio\n",
        "import cv2\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "import matplotlib.patches as patches\n",
        "import math\n",
        "from math import *\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "\n",
        "source_mat = ''\n",
        "#=r\"C:\\Users\\108652706\\Documents\\DOCUMENT_perso\\ISI\\MLA\\trials\\gaze360\"\n",
        "mat_fname = pjoin(source_mat, '/content/gaze360/metadata.mat')\n",
        "mat_contents = sio.loadmat(mat_fname)\n",
        "sorted(mat_contents.keys())\n"
      ],
      "id": "47784911",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__globals__',\n",
              " '__header__',\n",
              " '__version__',\n",
              " 'frame',\n",
              " 'gaze_dir',\n",
              " 'person_body_bbox',\n",
              " 'person_cam',\n",
              " 'person_eye_left_bbox',\n",
              " 'person_eye_right_bbox',\n",
              " 'person_eyes2d',\n",
              " 'person_eyes3d',\n",
              " 'person_face_bbox',\n",
              " 'person_head_bbox',\n",
              " 'person_identity',\n",
              " 'recording',\n",
              " 'recordings',\n",
              " 'split',\n",
              " 'splits',\n",
              " 'target_cam',\n",
              " 'target_pos2d',\n",
              " 'target_pos3d',\n",
              " 'ts']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "147d294c"
      },
      "source": [
        "#name= mat_contents['']\n",
        "\n",
        "recordings=mat_contents['recordings']\n",
        "recording = mat_contents['recording']\n",
        "person_identity= mat_contents['person_identity']\n",
        "frame= mat_contents['frame']\n",
        "person_head_bbox = mat_contents['person_head_bbox']\n",
        "person_eye_left_bbox= mat_contents['person_eye_left_bbox']\n",
        "cropType= 'head'\n",
        "person_face_bbox = mat_contents['person_face_bbox']\n",
        "target_pos3d=mat_contents['target_pos3d']\n",
        "person_eye_right_bbox= mat_contents['person_eye_right_bbox']\n",
        "gaze_dir =mat_contents['gaze_dir']"
      ],
      "id": "147d294c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBlRNz8gfZ_q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38fb4531-a844-4846-fec7-4fa8a6196fab"
      },
      "source": [
        "from torchvision import models \n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "from matplotlib import cm\n",
        "import torch\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "id": "nBlRNz8gfZ_q",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQN8xAH6fDxa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5341ec0e-1051-4ac3-c2b8-0b6287d8b050"
      },
      "source": [
        "!pip install face_alignment # used as second checker of eye detection in a head image"
      ],
      "id": "GQN8xAH6fDxa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: face_alignment in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from face_alignment) (4.1.2.30)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from face_alignment) (0.18.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from face_alignment) (1.19.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from face_alignment) (1.10.0+cu111)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from face_alignment) (4.62.3)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from face_alignment) (0.51.2)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.7/dist-packages (from face_alignment) (1.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->face_alignment) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->face_alignment) (0.34.0)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->face_alignment) (7.1.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->face_alignment) (1.2.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->face_alignment) (2021.11.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->face_alignment) (2.6.3)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->face_alignment) (3.2.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->face_alignment) (2.4.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->face_alignment) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->face_alignment) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->face_alignment) (3.0.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->face_alignment) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->face_alignment) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->face_alignment) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H54FANIMfBaY"
      },
      "source": [
        "import face_alignment\n",
        "from skimage import io\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import collections\n",
        "# Optionally set detector and some additional detector parameters\n",
        "face_detector = 'sfd'\n",
        "face_detector_kwargs = {\n",
        "    \"filter_threshold\" : 0.8\n",
        "}\n",
        "\n",
        "fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._3D, device='cuda', flip_input=True,\n",
        "                                  face_detector=face_detector, face_detector_kwargs=face_detector_kwargs)"
      ],
      "id": "H54FANIMfBaY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NU2JpqB5fiH1",
        "outputId": "0702231d-3940-4b3f-e606-75c953467752"
      },
      "source": [
        "resnet = models.resnet18(pretrained=True)\n",
        "resnet18 = nn.Sequential(*(list(resnet.children())[0:8])) #take 8 layers \n",
        "#resnet18.cpu()\n",
        "resnet18.to(device)"
      ],
      "id": "NU2JpqB5fiH1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (2): ReLU(inplace=True)\n",
              "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (5): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (6): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (7): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Nn-Z6DXnfDo"
      },
      "source": [
        "#create functions"
      ],
      "id": "2Nn-Z6DXnfDo"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFptR_exvSuT"
      },
      "source": [
        "# extract eyes from head image function\n",
        "def head_eye_extractors(imHead,fa,head_box,target,eye_left_box,eye_right_box):\n",
        "      if np.all(eye_left_box==np.array([-1,-1,-1,-1])) and np.all(eye_right_box==np.array([-1,-1,-1,-1])):\n",
        "          #print(\"eye_not_detected with Kellnhofer_al \")\n",
        "          imEye_l,imEye_r= bulat_al(imHead,fa)\n",
        "      else: \n",
        "          imEye_l,imEye_r= Kellnhofer_al_eyes(imHead,head_box,eye_left_box,eye_right_box)\n",
        "      x,y,z=target\n",
        "      yaw = atan(np.divide(z,x))\n",
        "      pitch=atan(np.divide(x,y))\n",
        "      h=[float(yaw),float(pitch)]\n",
        "      return h, imEye_l,imEye_r"
      ],
      "id": "EFptR_exvSuT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSx9kxK8G3WG"
      },
      "source": [
        "from torchvision import models \n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "from matplotlib import cm\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from torchvision import datasets\n",
        "import math\n",
        "#device = torch.device(\"cpu\")"
      ],
      "id": "YSx9kxK8G3WG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bR1XGTZtJ_XT",
        "outputId": "04c91e67-e38d-452e-f203-69a4f383c7b3"
      },
      "source": [
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "summary(resnet18,(3,224,224))"
      ],
      "id": "bR1XGTZtJ_XT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "              ReLU-7           [-1, 64, 56, 56]               0\n",
            "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
            "             ReLU-10           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
            "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
            "             ReLU-14           [-1, 64, 56, 56]               0\n",
            "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
            "             ReLU-17           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
            "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
            "             ReLU-21          [-1, 128, 28, 28]               0\n",
            "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
            "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
            "             ReLU-26          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
            "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
            "             ReLU-30          [-1, 128, 28, 28]               0\n",
            "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
            "             ReLU-33          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
            "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
            "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
            "             ReLU-37          [-1, 256, 14, 14]               0\n",
            "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
            "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
            "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
            "             ReLU-42          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
            "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
            "             ReLU-46          [-1, 256, 14, 14]               0\n",
            "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
            "             ReLU-49          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
            "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
            "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-53            [-1, 512, 7, 7]               0\n",
            "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
            "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
            "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-58            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
            "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-62            [-1, 512, 7, 7]               0\n",
            "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-65            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
            "================================================================\n",
            "Total params: 11,176,512\n",
            "Trainable params: 11,176,512\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 62.78\n",
            "Params size (MB): 42.64\n",
            "Estimated Total Size (MB): 105.99\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W65usKpHbOYu"
      },
      "source": [
        "#form datasets"
      ],
      "id": "W65usKpHbOYu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoCZyi5Ib-u1"
      },
      "source": [
        "def dataset ( source_p,nb_image):\n",
        "    head_imgs=[]\n",
        "    l_imgs=[]\n",
        "    r_imgs=[]\n",
        "    heads=[]\n",
        "    h_list=[]\n",
        "    gaze=[]\n",
        "    for i in range(nb_image): \n",
        "          imHead = cv2.imread(os.path.join(\n",
        "            source_p,\n",
        "            recordings[0,recording[0,i]][0],\n",
        "            cropType,\n",
        "            '%06d' % person_identity[0,i],\n",
        "            '%06d.jpg' % frame[0,i]\n",
        "            ))\n",
        "          heads.append(cv2.resize(imHead,(224,224)))\n",
        "          h,imEye_l,imEye_r= head_eye_extractors(imHead,fa,person_head_bbox[i,:],target_pos3d[i],person_eye_left_bbox[i,:],person_eye_right_bbox[i,:])\n",
        "          l_imgs.append(imEye_l)\n",
        "          r_imgs.append(imEye_r)\n",
        "          h_list.append(h)\n",
        "          gaze.append(gaze_dir[i])\n",
        "    return l_imgs,r_imgs,h_list,heads,gaze\n",
        "\n"
      ],
      "id": "JoCZyi5Ib-u1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlhKjcFD8CJb"
      },
      "source": [
        "source_p=('/content/gaze360/imgs') #path to dataset\n",
        "nb_image= len(frame[0])# number of images\n",
        "nb_image = 10000 #trial with 20 images only\n",
        "l_imgs,r_imgs,h,heads,gazes=dataset ( source_p,nb_image)"
      ],
      "id": "NlhKjcFD8CJb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "8-0v7HT3Rh1Z",
        "outputId": "1114963c-25c4-4555-f886-ecf91d7cde7b"
      },
      "source": [
        "import pickle\n",
        "# Save \n",
        "\"\"\"\n",
        "with open(\"/content/drive/My Drive/l_imgs.txt\", \"wb\") as fp:   #Pickling\n",
        "    pickle.dump(l_imgs, fp)\n",
        "\n",
        "with open(\"/content/drive/My Drive/r_imgs.txt\", \"wb\") as fp:   #Pickling\n",
        "    pickle.dump(r_imgs, fp)\n",
        "\n",
        "with open(\"/content/drive/My Drive/heads.txt\", \"wb\") as fp:   #Pickling\n",
        "    pickle.dump(heads, fp)\n",
        "\n",
        "with open(\"/content/drive/My Drive/gaze.txt\", \"wb\") as fp:   #Pickling\n",
        "    pickle.dump(gazes, fp)\n",
        "\n",
        "with open(\"/content/drive/My Drive/h_list.txt\", \"wb\") as fp:   #Pickling\n",
        "    pickle.dump(h, fp)\n",
        "\"\"\""
      ],
      "id": "8-0v7HT3Rh1Z",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nwith open(\"/content/drive/My Drive/l_imgs.txt\", \"wb\") as fp:   #Pickling\\n    pickle.dump(l_imgs, fp)\\n\\nwith open(\"/content/drive/My Drive/r_imgs.txt\", \"wb\") as fp:   #Pickling\\n    pickle.dump(r_imgs, fp)\\n\\nwith open(\"/content/drive/My Drive/heads.txt\", \"wb\") as fp:   #Pickling\\n    pickle.dump(heads, fp)\\n\\nwith open(\"/content/drive/My Drive/gaze.txt\", \"wb\") as fp:   #Pickling\\n    pickle.dump(gazes, fp)\\n\\nwith open(\"/content/drive/My Drive/h_list.txt\", \"wb\") as fp:   #Pickling\\n    pickle.dump(h, fp)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gM0qZdMvSajZ"
      },
      "source": [
        "import pickle\n",
        "# Import \n",
        "with open(\"/content/drive/My Drive/l_imgs.txt\", \"rb\") as fp:   # Unpickling\n",
        "    l_imgs = pickle.load(fp)\n",
        "\n",
        "with open(\"/content/drive/My Drive/r_imgs.txt\", \"rb\") as fp:   # Unpickling\n",
        "    r_imgs = pickle.load(fp)\n",
        "\n",
        "\n",
        "with open(\"/content/drive/My Drive/gaze.txt\", \"rb\") as fp:   # Unpickling\n",
        "    gazes = pickle.load(fp)\n",
        "\n",
        "\n"
      ],
      "id": "gM0qZdMvSajZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBz895ocw-TH"
      },
      "source": [
        "with open(\"/content/drive/My Drive/h_list.txt\", \"rb\") as fp:   # Unpickling\n",
        "    h = pickle.load(fp)"
      ],
      "id": "zBz895ocw-TH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9vDnpPXwpkH"
      },
      "source": [
        "import pickle\n",
        "with open(\"/content/drive/My Drive/heads.txt\", \"rb\") as fp:   # Unpickling\n",
        "    heads = pickle.load(fp)"
      ],
      "id": "h9vDnpPXwpkH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjWR6tFdNzH9"
      },
      "source": [
        "'''\n",
        "l_imgs is fl \n",
        "r_imgs is fr \n",
        "h is (yaw,pich) list \n",
        "heads is heads dataset\n",
        "gazes is (gx,gy,gz) a.k.a target \n",
        "'''"
      ],
      "id": "qjWR6tFdNzH9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9hiKiKjJ3jt"
      },
      "source": [
        "## Eye prediction"
      ],
      "id": "m9hiKiKjJ3jt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7qcFtNU0BTk"
      },
      "source": [
        "class eyeDataset(Dataset):\n",
        "    \"\"\"eye landmark dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, imgs, transform=None):\n",
        "       \n",
        "        self.imgs = imgs\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        #img_name = os.path.join(self.root_dir,self.landmarks_frame.iloc[idx, 0])\n",
        "        image = self.imgs[idx] #io.imread(img_name)\n",
        "        image = Image.fromarray(image)\n",
        "        sample = {'image': image}\n",
        "\n",
        "        if self.transform:\n",
        "            sample['image'] = self.transform(sample['image'])\n",
        "\n",
        "        return sample"
      ],
      "id": "Z7qcFtNU0BTk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGkxVIzcOkbo"
      },
      "source": [
        "preprocess = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )])"
      ],
      "id": "kGkxVIzcOkbo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvTrWEqXiSEY"
      },
      "source": [
        "l_imgs = l_imgs[:8600]\n",
        "r_imgs = r_imgs [:8600]"
      ],
      "id": "fvTrWEqXiSEY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32DgbHP00hJn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d800f5c-16fe-45b9-a82d-cb354839b1e6"
      },
      "source": [
        "l_eye_set=eyeDataset(imgs=l_imgs,transform=preprocess)\n",
        "l_eye_loader = DataLoader(l_eye_set, batch_size=200,\n",
        "                        shuffle=False, num_workers=10)\n",
        "\n",
        "features_left =torch.zeros((1,2048), dtype=torch.int32, device = 'cuda')\n",
        "\n",
        "for i in range(len(l_eye_loader)):\n",
        "    l=next(iter(l_eye_loader))\n",
        "    outputs_l=l['image'].to(device)\n",
        "    left=resnet18(outputs_l).flatten(start_dim=1)\n",
        "    features_left = torch.cat((features_left,left), 0)\n",
        "\n",
        "features_left=features_left[1:,:]\n"
      ],
      "id": "32DgbHP00hJn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr42yaG5j_yf"
      },
      "source": [
        "# Save \n",
        "with open(\"/content/drive/My Drive/features_left.txt\", \"wb\") as fp:   #Pickling\n",
        "    pickle.dump(features_left, fp)"
      ],
      "id": "Vr42yaG5j_yf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lz3az7W0kDte"
      },
      "source": [
        "# Import \n",
        "with open(\"/content/drive/My Drive/features_left.txt\", \"rb\") as fp:   # Unpickling\n",
        "    features_left = pickle.load(fp)\n"
      ],
      "id": "lz3az7W0kDte",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTBOQAEx9n_9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beff3c86-6c84-4b46-fa10-e36240884584"
      },
      "source": [
        "r_eye_set=eyeDataset(imgs=r_imgs,transform=preprocess)\n",
        "r_eye_loader = DataLoader(r_eye_set, batch_size=200,\n",
        "                        shuffle=False, num_workers=10)\n",
        "resnet18.eval()\n",
        "features_right =torch.zeros((1,2048), dtype=torch.int32, device = 'cuda')\n",
        "for i in range(len(r_eye_loader)):\n",
        "    r=next(iter(r_eye_loader))\n",
        "    outputs_r=r['image'].to(device)\n",
        "    right=resnet18(outputs_r).flatten(start_dim=1)# yields a tensor of size([batch_size, 2048])\n",
        "    features_right = torch.cat((features_right,right), 0)\n",
        "\n",
        "features_right=features_right[1:,:]"
      ],
      "id": "qTBOQAEx9n_9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koRmeRbheN8x"
      },
      "source": [
        "\n",
        "\n",
        "with open(\"/content/drive/My Drive/features_right.txt\", \"wb\") as fp:   #Pickling\n",
        "    pickle.dump(features_right, fp)"
      ],
      "id": "koRmeRbheN8x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTv0zqsphWPf"
      },
      "source": [
        "\n",
        "with open(\"/content/drive/My Drive/features_right.txt\", \"rb\") as fp:   # Unpickling\n",
        "    features_right = pickle.load(fp)"
      ],
      "id": "cTv0zqsphWPf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwp_Pw8zjpF7"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "id": "vwp_Pw8zjpF7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiXZ3OsSO94U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "def1d54b-9f33-4df1-c2d7-29f507eb63ff"
      },
      "source": [
        "features_right.shape"
      ],
      "id": "aiXZ3OsSO94U",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10000, 2048])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDkjZSJE6o5v"
      },
      "source": [
        "from torchsummary import summary\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "summary(resnet18.to(device),input_size=(3, 36, 60))# summary works only with gpu,so we change model to gpu then take it back to cpu\n",
        "device= torch.device('cpu')\n"
      ],
      "id": "JDkjZSJE6o5v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2SOUwp_MC53"
      },
      "source": [
        "##Head pose"
      ],
      "id": "G2SOUwp_MC53"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AM2Ahecpzden"
      },
      "source": [
        "\n",
        "class headposeDataset(Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, imgs, h, transform=None):\n",
        "        self.h = h\n",
        "        self.imgs = imgs\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        image = self.imgs[idx] \n",
        "        image = Image.fromarray(image)\n",
        "        ex = self.h[idx]\n",
        "        '''\n",
        "        landmarks = np.zeros(2)\n",
        "        landmarks=np.zeros(2)\n",
        "        landmarks[0]= ex[0]\n",
        "        landmarks[1]= ex[1]\n",
        "        '''\n",
        "        landmarks = np.array(ex)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        #sample = {'image': image, 'landmarks': landmarks}\n",
        "        sample = {'image': image, 'landmarks': landmarks}\n",
        "\n",
        "        return sample"
      ],
      "id": "AM2Ahecpzden",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Td_uM91TUFZ"
      },
      "source": [
        "preprocess = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.double()),\n",
        "        transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )])\n",
        "face_pose_dataset = headposeDataset(imgs=heads,h=h,transform=preprocess)\n",
        "dataloader = DataLoader(face_pose_dataset, batch_size=32,\n",
        "                        shuffle=True, num_workers=0)\n"
      ],
      "id": "1Td_uM91TUFZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOH0ar_j_Iz6"
      },
      "source": [
        "validation_split = .2\n",
        "shuffle_dataset = True\n",
        "random_seed= 42\n",
        "batch_size= 32\n",
        "\n",
        "# Creating data indices for training and validation splits:\n",
        "dataset_size = len(face_pose_dataset)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(validation_split * dataset_size))\n",
        "if shuffle_dataset :\n",
        "    np.random.seed(random_seed)\n",
        "    np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "# Creating PT data samplers and loaders:\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(face_pose_dataset, batch_size=batch_size, \n",
        "                                           sampler=train_sampler)\n",
        "validation_loader = torch.utils.data.DataLoader(face_pose_dataset, batch_size=batch_size,\n",
        "                                                sampler=valid_sampler)"
      ],
      "id": "aOH0ar_j_Iz6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZT7JvGhIv4_"
      },
      "source": [
        ""
      ],
      "id": "EZT7JvGhIv4_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm1WqwHufTrj"
      },
      "source": [
        "#Function to create a Pretrained Resnet Model with a Fully Connected Layer\n",
        "# (out_1: ouput of 1st layer), (out_2: ouput of 2nd layer)\n",
        "def createModel_resnet34(out_1, out_2):\n",
        "    model = torchvision.models.resnet34(pretrained = True)\n",
        "    num_ftrs = model.fc.in_features\n",
        "\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    \n",
        "    #Creating 3 Linear connected layers which can be trained\n",
        "    fc1 = nn.Linear(num_ftrs, out_1)\n",
        "    fc2 = nn.Linear(out_1, out_2)\n",
        "    fc3 = nn.Linear(out_2, 2)\n",
        "\n",
        "    layers = [fc1, fc2, fc3]\n",
        "    for linearLayer in layers:\n",
        "        #Applying He initialization to all layers\n",
        "        nn.init.kaiming_uniform_(linearLayer.weight, nonlinearity='leaky_relu')\n",
        "  \n",
        "\n",
        "\n",
        "    #Setting Resnet's fully connected layer to our collection of three Linear layers with nn.Sequential\n",
        "    model.fc = nn.Sequential(fc1, nn.LeakyReLU(),fc2, nn.LeakyReLU(),fc3)\n",
        "    model.double() #double to set variables to double\n",
        "    #Sending the device to the GPU if avaliable\n",
        "    model.to(device)\n",
        "\n",
        "    return model"
      ],
      "id": "rm1WqwHufTrj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maNWaV-7SXya",
        "outputId": "9955eb75-32d2-4609-df30-ae88f648e937"
      },
      "source": [
        "#check types : make sure both types are the same\n",
        "print(next(iter(train_loader))['image'].type())\n",
        "print(next(iter(train_loader))['landmarks'].type())\n"
      ],
      "id": "maNWaV-7SXya",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.DoubleTensor\n",
            "torch.DoubleTensor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "YXDtuRZ1G04n",
        "outputId": "260e277b-bf15-47e4-cf4c-f6253c8177aa"
      },
      "source": [
        "optimizer = optim.SGD(model.parameters())"
      ],
      "id": "YXDtuRZ1G04n",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-3cbe170fa47d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIiEKZs2RWr_",
        "outputId": "ce856e5d-d93f-4fef-dbd7-d71de4cde287"
      },
      "source": [
        "#Function to handle training\n",
        "#(model: model to train), (criterion: loss function for the model), (optimizer, what optimizer to use)\n",
        "def train(model, criterion, optimizer, file, epochs=30, trainLoss=[], validationLoss=[]):\n",
        "    #device = torch.device(\"cpu\")\n",
        "    print(\"Starting training\")\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch Number: {epoch}\")\n",
        "\n",
        "        testAccuracy = 0\n",
        "        totalLoss = 0\n",
        "\n",
        "        for i in range(len(train_loader)):\n",
        "            #Put model into training mode\n",
        "            model.train()\n",
        "            batch= next(iter(train_loader))\n",
        "            x=batch['image'].to(device)\n",
        "            y=batch['landmarks'].to(device)\n",
        "            #Zero out gradients\n",
        "            optimizer.zero_grad()\n",
        "            #Make a prediction\n",
        "            y_hat = model(x)\n",
        "            \n",
        "            #Calculate the loss\n",
        "            loss = criterion(y_hat, y)\n",
        "            l = torch.sqrt(loss)\n",
        "            #Update the total loss\n",
        "            totalLoss += l.item()\n",
        "            #Initiate backpropagation\n",
        "            l.backward()\n",
        "            \n",
        "            #Update parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            #testAccuracy += (y_hat == y).sum().item()\n",
        "        trainLoss.append(totalLoss)\n",
        "        print(f\"loss: {totalLoss}\")\n",
        "        \n",
        "        valAccuracy = 0\n",
        "        totalLoss = 0\n",
        "        \n",
        "        \n",
        "        for j in range(len(validation_loader)):\n",
        "            # Put the model into evaluation mode\n",
        "            banch= next(iter(validation_loader))\n",
        "            x=banch['image'].to(device)\n",
        "            y=banch['landmarks'].to(device)\n",
        "            model.eval()\n",
        "            y_pred= model(x)\n",
        "            #print(y_pred)\n",
        "            loss = criterion(y_pred, y)\n",
        "            l = torch.sqrt(loss)\n",
        "            totalLoss += l\n",
        "        print(f\"val loss: {totalLoss}\")\n",
        "        validationLoss.append(totalLoss)\n",
        "        #Save our model for each epoch\n",
        "    fig = plt.figure(figsize=(20,10))\n",
        "    plt.title(\"Train-Validation rmse\")\n",
        "    plt.plot(trainLoss, label='train')\n",
        "    plt.plot(validationLoss, label='validation')\n",
        "    plt.xlabel('num_epochs', fontsize=12)\n",
        "    plt.ylabel('RMSE', fontsize=12)\n",
        "    plt.legend(loc='best')\n",
        "    plt.savefig('RMSE.png')\n",
        "\n",
        "\n",
        "    torch.save(model.state_dict(), file)\n",
        "    torch.save(model.state_dict(), \"/content/drive/MyDrive/\"+file)\n",
        "    return trainLoss, validationLoss, model\n",
        "\n",
        "#Create a model using our function\n",
        "model = createModel_resnet34(512, 128)\n",
        "#Define our loss and optimizers\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(),lr=0.001)\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
        "#optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
        "#Specify output file name\n",
        "file = \"ResnetFinal.pth\" #save model\n",
        "\n",
        "#Try to use Colab's GPU here, otherwise it will take a long time to train.\n",
        "output = train(model, criterion, optimizer, file)"
      ],
      "id": "BIiEKZs2RWr_",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training\n",
            "Epoch Number: 0\n",
            "loss: 151.75080912735532\n",
            "val loss: 33.17933365506039\n",
            "Epoch Number: 1\n",
            "loss: 120.2635108806747\n",
            "val loss: 29.722391648607104\n",
            "Epoch Number: 2\n",
            "loss: 108.98237303671603\n",
            "val loss: 27.87961746200589\n",
            "Epoch Number: 3\n",
            "loss: 102.56988513685268\n",
            "val loss: 26.1715686066709\n",
            "Epoch Number: 4\n",
            "loss: 99.1666793359327\n",
            "val loss: 25.953395890868084\n",
            "Epoch Number: 5\n",
            "loss: 96.20010361654566\n",
            "val loss: 24.796526867359187\n",
            "Epoch Number: 6\n",
            "loss: 93.02817858683137\n",
            "val loss: 24.668390215020512\n",
            "Epoch Number: 7\n",
            "loss: 92.93780227055706\n",
            "val loss: 23.717579312856827\n",
            "Epoch Number: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u54m2qfei_gY"
      },
      "source": [
        "with open('/content/gdrive/My Drive/heads.txt', 'w') as f:\n",
        "  heads.write('content')"
      ],
      "id": "u54m2qfei_gY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjQjzjBhBHOa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b88b8f4-1819-4226-889a-2a7eb2efc444"
      },
      "source": [
        "%reset # to reset all variables"
      ],
      "id": "hjQjzjBhBHOa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n",
            "Don't know how to reset  #, please run `%reset?` for details\n",
            "Don't know how to reset  to, please run `%reset?` for details\n",
            "Don't know how to reset  reset, please run `%reset?` for details\n",
            "Don't know how to reset  all, please run `%reset?` for details\n",
            "Don't know how to reset  variables, please run `%reset?` for details\n"
          ]
        }
      ]
    }
  ]
}